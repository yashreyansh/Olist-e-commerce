# Olist E-Commerce Data Pipeline

A production-grade end-to-end data engineering pipeline for processing and analyzing Brazilian e-commerce data from Olist. Built with Apache Spark, Delta Lake, Apache Airflow, and PostgreSQL.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.x-orange.svg)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-2.x-green.svg)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.x-red.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14+-blue.svg)

## üìã Table of Contents
- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Setup Instructions](#setup-instructions)
- [Usage](#usage)
- [Data Flow](#data-flow)
- [Monitoring & Logging](#monitoring--logging)
- [Contributing](#contributing)

## üéØ Overview

This project implements a scalable data pipeline for processing Brazilian e-commerce order and payment data. The pipeline handles:
- **Incremental data processing** from raw parquet files
- **Delta Lake tables** with Change Data Feed (CDF) for efficient tracking
- **Automated synchronization** to PostgreSQL for analytics
- **Full audit trail** with comprehensive logging
- **Orchestration** via Apache Airflow

## üèóÔ∏è Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Landing Zone   ‚îÇ  Raw Parquet Files
‚îÇ  (Order/Payment)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LoadToSilver   ‚îÇ  PySpark Processing
‚îÇ     Script      ‚îÇ  - Data Transformation
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - Schema Validation
         ‚îÇ           - Audit Fields
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Delta Lake    ‚îÇ  Silver Layer
‚îÇ  (Payment Fact) ‚îÇ  - Payment Facts
‚îÇ (Order Summary) ‚îÇ  - Order Summary
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - CDF Enabled
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SyncToPostgres  ‚îÇ  Incremental Sync
‚îÇ     Script      ‚îÇ  - CDF-based
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - UPSERT Logic
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL    ‚îÇ  Analytics Layer
‚îÇ  (OLIST Schema) ‚îÇ  - Payment Facts
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - Order Summary
                     - Sync Tracking
                     - Audit Logs
```

## ‚ú® Features

### Data Processing
- ‚úÖ **Incremental Processing**: Only processes new/changed data
- ‚úÖ **Delta Lake Integration**: ACID transactions with time-travel capabilities
- ‚úÖ **Change Data Feed (CDF)**: Efficient tracking of data changes
- ‚úÖ **Schema Evolution**: Handles schema changes gracefully
- ‚úÖ **Data Deduplication**: Prevents duplicate records

### Data Quality
- ‚úÖ **Audit Fields**: Tracks source files, job IDs, timestamps
- ‚úÖ **Data Validation**: Type casting and null handling
- ‚úÖ **Composite Keys**: Supports multi-column primary keys
- ‚úÖ **UPSERT Logic**: Smart insert/update handling in PostgreSQL

### Operational Excellence
- ‚úÖ **Comprehensive Logging**: Audit trail for all operations
- ‚úÖ **Error Handling**: Graceful failure recovery
- ‚úÖ **File Archiving**: Processed files moved to archive
- ‚úÖ **Version Tracking**: Sync state management

## üõ†Ô∏è Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Processing** | Apache Spark (PySpark) | Distributed data processing |
| **Storage** | Delta Lake | ACID-compliant data lake |
| **Database** | PostgreSQL | Analytics and reporting |
| **Orchestration** | Apache Airflow | Workflow management |
| **Language** | Python 3.8+ | Primary programming language |

## üìÅ Project Structure
```
Olist-e-commerce/
‚îú‚îÄ‚îÄ dags/                           # Airflow DAG definitions
‚îÇ   ‚îú‚îÄ‚îÄ olist_file_processing.py    # File processing orchestration
‚îÇ   ‚îî‚îÄ‚îÄ olist_postgres_sync.py      # Postgres sync orchestration
‚îÇ
‚îú‚îÄ‚îÄ gamma/Scripts/                  # Core processing scripts
‚îÇ   ‚îú‚îÄ‚îÄ LoadToSilver.py            # Landing ‚Üí Delta Lake
‚îÇ   ‚îú‚îÄ‚îÄ SyncToPostgres.py          # Delta Lake ‚Üí Postgres
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ dependencies/               # Utility modules
‚îÇ       ‚îú‚îÄ‚îÄ spark.py               # Spark session management
‚îÇ       ‚îú‚îÄ‚îÄ Add_log.py             # Audit logging
‚îÇ       ‚îî‚îÄ‚îÄ archieveFiles.py       # File archiving
‚îÇ
‚îú‚îÄ‚îÄ data/Olist_e-commerce/         # Data directories
‚îÇ   ‚îú‚îÄ‚îÄ Order_chunks/              # Raw order files
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processed/             # Archived orders
‚îÇ   ‚îú‚îÄ‚îÄ Payment_chunks/            # Raw payment files
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processed/             # Archived payments
‚îÇ   ‚îî‚îÄ‚îÄ SilverLayer/               # Delta tables
‚îÇ       ‚îú‚îÄ‚îÄ Payment_fact/          # Payment transactions
‚îÇ       ‚îî‚îÄ‚îÄ Order_summary/         # Order aggregates
‚îÇ
‚îú‚îÄ‚îÄ sql/                           # Database schemas
‚îÇ   ‚îú‚îÄ‚îÄ create_tables.sql          # PostgreSQL table definitions
‚îÇ   ‚îî‚îÄ‚îÄ indexes.sql                # Performance indexes
‚îÇ
‚îú‚îÄ‚îÄ README.md                      # This file
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies
```

## üöÄ Setup Instructions

### Prerequisites
- Python 3.8 or higher
- Apache Spark 3.x
- PostgreSQL 14+
- Apache Airflow 2.x
- 8GB+ RAM recommended

### 1. Clone Repository
```bash
git clone https://github.com/yashreyansh/Olist-e-commerce.git
cd Olist-e-commerce
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

**requirements.txt:**
```txt
pyspark==3.4.0
delta-spark==2.4.0
psycopg2-binary==2.9.6
apache-airflow==2.6.0
pandas==2.0.0
```

### 3. Configure PostgreSQL
```bash
# Create database
psql -U postgres -c "CREATE DATABASE data_db;"

# Run schema setup
psql -U postgres -d data_db -f sql/create_tables.sql
```

**sql/create_tables.sql:**
```sql
-- Create schema
CREATE SCHEMA IF NOT EXISTS OLIST;

-- Payment Facts
CREATE TABLE OLIST.payment_facts (
    order_id VARCHAR NOT NULL,
    payment_sequential BIGINT NOT NULL,
    payment_installments INTEGER,
    payment_type VARCHAR,
    payment_value FLOAT,
    proc_run_id VARCHAR,
    source_payment_file VARCHAR,
    created_on TIMESTAMP,
    updated_on TIMESTAMP,
    PRIMARY KEY (order_id, payment_sequential)
);

-- Order Summary
CREATE TABLE OLIST.order_summary (
    order_id VARCHAR PRIMARY KEY,
    customer_id VARCHAR,
    order_status VARCHAR,
    order_approved_at TIMESTAMP,
    order_purchase_timestamp TIMESTAMP,
    order_delivered_carrier_date TIMESTAMP,
    order_delivered_customer_date TIMESTAMP,
    order_estimated_delivery_date TIMESTAMP,
    no_of_payments BIGINT,
    payment_installments INTEGER,
    payment_types_used VARCHAR,
    total_payment_value FLOAT,
    created_by_proc_run_id VARCHAR,
    updated_by_proc_run_id VARCHAR,
    source_order_file VARCHAR,
    source_payment_file VARCHAR,
    created_on TIMESTAMP,
    updated_on TIMESTAMP
);

-- Staging Tables
CREATE TABLE OLIST.payment_facts_staging (LIKE OLIST.payment_facts);
CREATE TABLE OLIST.order_summary_staging (LIKE OLIST.order_summary);

-- Sync Tracking
CREATE TABLE OLIST.sync_tracking (
    table_name VARCHAR PRIMARY KEY,
    last_version BIGINT,
    last_sync_time TIMESTAMP,
    run_id VARCHAR
);

-- Audit Log
CREATE TABLE OLIST.audit_log (
    audit_id SERIAL PRIMARY KEY,
    event_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    run_id VARCHAR(50),
    event_type VARCHAR(100),
    status VARCHAR(20),
    resp_message VARCHAR(500),
    CONSTRAINT valid_status CHECK (status IN ('Started', 'In-Progress', 'Completed', 'Failed'))
);

-- Indexes
CREATE INDEX idx_audit_run_id ON OLIST.audit_log(run_id);
CREATE INDEX idx_audit_time ON OLIST.audit_log(event_time DESC);
CREATE INDEX idx_payment_order ON OLIST.payment_facts(order_id);
```

### 4. Configure Connection Settings

Update `SyncToPostgres.py`:
```python
postgres_config = {
    'host': 'localhost',     # Your Postgres host
    'port': 5432,
    'database': 'data_db',
    'user': 'your_user',
    'password': 'your_password'
}
```

### 5. Set Up Airflow (Optional)
```bash
# Initialize Airflow
export AIRFLOW_HOME=~/airflow
airflow db init

# Create admin user
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# Copy DAGs
cp dags/*.py $AIRFLOW_HOME/dags/

# Start services
airflow webserver --port 8080 &
airflow scheduler &
```

## üíª Usage

### Manual Execution

#### 1. Process Files to Delta Lake
```bash
python gamma/Scripts/LoadToSilver.py
```

**What it does:**
- Reads parquet files from landing zone
- Transforms and validates data
- Merges into Delta Lake tables
- Archives processed files
- Adds audit metadata

#### 2. Sync Delta to PostgreSQL
```bash
python gamma/Scripts/SyncToPostgres.py
```

**What it does:**
- Reads changes from Delta (CDF)
- Writes to PostgreSQL staging
- Executes UPSERT operations
- Updates sync tracking

### Airflow Execution

Access Airflow UI: `http://localhost:8080`

**Available DAGs:**
- `olist_file_processing` - Runs LoadToSilver.py
- `olist_postgres_sync` - Runs SyncToPostgres.py

Trigger manually or set schedule:
```python
# In DAG file
schedule_interval='*/15 * * * *'  # Every 15 minutes
```

## üîÑ Data Flow

### Phase 1: Landing ‚Üí Delta Lake
```python
# Example: Processing order file
Order File (10.parquet)
    ‚Üì Read & Transform
    ‚îú‚îÄ Add proc_run_id: "scheduled_2026-01-08T04:40:00"
    ‚îú‚îÄ Add source_order_file: "10.parquet"
    ‚îú‚îÄ Add created_on: current_timestamp()
    ‚îú‚îÄ Convert timestamps
    ‚Üì Merge to Delta
    ‚îî‚îÄ Payment_fact v2 (50 new records)
```

### Phase 2: Delta Lake ‚Üí PostgreSQL
```python
# Example: Incremental sync
Delta Table (v2)
    ‚Üì Read CDF (v1 ‚Üí v2)
    ‚îú‚îÄ 50 changed records
    ‚îú‚îÄ Filter: INSERT + UPDATE_POSTIMAGE
    ‚Üì Write to Staging
    ‚îú‚îÄ TRUNCATE staging
    ‚îú‚îÄ INSERT 50 records
    ‚Üì UPSERT to Main
    ‚îú‚îÄ 30 INSERTs (new)
    ‚îú‚îÄ 20 UPDATEs (existing)
    ‚îî‚îÄ Update sync_tracking: last_version = 2
```

## üìä Monitoring & Logging

### Query Audit Logs
```sql
-- Recent sync jobs
SELECT run_id, event_type, status, resp_message, event_time
FROM OLIST.audit_log
WHERE event_time > NOW() - INTERVAL '1 day'
ORDER BY event_time DESC;

-- Failed operations
SELECT * FROM OLIST.audit_log
WHERE status = 'Failed'
ORDER BY event_time DESC;
```

### Check Sync Status
```sql
-- Sync tracking
SELECT table_name, last_version, last_sync_time
FROM OLIST.sync_tracking;

-- Record counts
SELECT 
    'payment_facts' as table_name,
    COUNT(*) as record_count
FROM OLIST.payment_facts
UNION ALL
SELECT 
    'order_summary',
    COUNT(*)
FROM OLIST.order_summary;
```

### Delta Lake Metrics
```python
from delta.tables import DeltaTable

# Check versions
delta_table = DeltaTable.forPath(spark, "/path/to/delta")
delta_table.history().show()

# View changes
changes = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 0) \
    .load("/path/to/delta")
changes.groupBy("_change_type").count().show()
```

## üéØ Key Design Patterns

### 1. Change Data Feed (CDF)
- Only syncs changed records (not full table)
- Reduces network/processing overhead
- Enables time-travel queries

### 2. UPSERT Pattern
```sql
INSERT INTO target_table
SELECT * FROM staging_table
ON CONFLICT (primary_key)
DO UPDATE SET column = EXCLUDED.column;
```

### 3. Audit Trail
Every operation logged with:
- Unique run_id
- Source file names
- Processing timestamps
- Success/failure status

### 4. Idempotent Processing
- Can re-run safely
- Handles duplicates
- Version-based sync tracking

## üêõ Troubleshooting

### Issue: "Change Data Feed not enabled"
**Solution:**
```python
# Enable CDF when creating Delta table
.option("delta.enableChangeDataFeed", "true")
```

### Issue: Duplicate records in Delta
**Solution:**
```python
# Add deduplication before merge
payment_df = payment_df.dropDuplicates(["order_id", "payment_sequential"])
```

### Issue: Version mismatch error
**Solution:**
```sql
-- Reset sync tracking
UPDATE OLIST.sync_tracking 
SET last_version = 0 
WHERE table_name = 'your_table';
```

## üìà Performance Tips

1. **Partition Delta tables** for large datasets:
```python
.partitionBy("year", "month")
```

2. **Optimize Delta tables** regularly:
```python
delta_table.optimize().executeCompaction()
```

3. **Vacuum old versions** (after 7 days):
```python
delta_table.vacuum(168)  # hours
```

4. **Index PostgreSQL** frequently queried columns:
```sql
CREATE INDEX idx_order_date ON OLIST.order_summary(order_purchase_timestamp);
```

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## üìù License

This project is licensed under the MIT License.

## üë§ Author

**Yash Reyansh**
- GitHub: [@yashreyansh](https://github.com/yashreyansh)

## üôè Acknowledgments

- Olist for providing the Brazilian e-commerce dataset
- Apache Spark & Delta Lake communities
- Contributors and reviewers

---

**‚≠ê Star this repo if you find it helpful!**
